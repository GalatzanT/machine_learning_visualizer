"""
Service pentru generare explicaÈ›ii automate
AnalizeazÄƒ starea algoritmului È™i genereazÄƒ text explicativ
"""
import numpy as np
from typing import List


class ExplanationService:
    """Service pentru generare explicaÈ›ii despre procesul de Ã®nvÄƒÈ›are"""
    
    @staticmethod
    def generate_step_explanation(
        x: np.ndarray,
        y: np.ndarray,
        y_pred: np.ndarray,
        w: float,
        b: float,
        dw: float,
        db: float,
        lr: float,
        errors: np.ndarray
    ) -> List[str]:
        """
        GenereazÄƒ explicaÈ›ii textuale despre ce se Ã®ntÃ¢mplÄƒ Ã®n acest pas.
        """
        explanations = []
        n = len(x)
        
        # 1. AnalizÄƒ poziÈ›ionare linie vs puncte
        positive_errors = np.sum(errors > 0)
        negative_errors = np.sum(errors < 0)
        
        if positive_errors > negative_errors * 1.5:
            explanations.append(f"ğŸ”´ Linia este SUB majoritatea punctelor ({positive_errors}/{n})")
            explanations.append("â¡ï¸ Bias-ul (b) va CREÈ˜TE pentru a ridica linia")
        elif negative_errors > positive_errors * 1.5:
            explanations.append(f"ğŸ”µ Linia este PESTE majoritatea punctelor ({negative_errors}/{n})")
            explanations.append("â¡ï¸ Bias-ul (b) va SCÄ‚DEA pentru a coborÃ® linia")
        else:
            explanations.append("ğŸŸ¢ Linia este relativ echilibratÄƒ faÈ›Äƒ de puncte")
        
        # 2. AnalizÄƒ gradient w (pantÄƒ)
        if abs(dw) > 0.1:
            direction = "SCADÄ‚" if dw > 0 else "CREASCÄ‚"
            explanations.append(f"ğŸ“Š Panta trebuie sÄƒ {direction} (gradient w = {dw:.3f})")
        else:
            explanations.append("âœ… Panta este aproape optimÄƒ")
        
        # 3. AnalizÄƒ gradient b
        if abs(db) > 0.1:
            direction = "SCÄ‚DEA" if db > 0 else "CREÈ˜TE"
            explanations.append(f"â¬†ï¸â¬‡ï¸ Interceptul va {direction} (gradient b = {db:.3f})")
        
        # 4. AnalizÄƒ learning rate
        step_size = lr * abs(dw) + lr * abs(db)
        if step_size > 1.0:
            explanations.append("âš ï¸ ATENÈšIE: Learning rate prea mare â†’ paÈ™i foarte mari!")
            explanations.append("Riscul de oscilaÈ›ie sau divergenÈ›Äƒ este crescut")
        elif step_size < 0.001:
            explanations.append("ğŸŒ Learning rate mic â†’ progres foarte lent")
        
        # 5. IdentificÄƒ punctele problematice
        error_magnitudes = np.abs(errors)
        max_error_idx = np.argmax(error_magnitudes)
        max_error = error_magnitudes[max_error_idx]
        
        if max_error > np.mean(error_magnitudes) * 2:
            explanations.append(f"â­ Punctul {max_error_idx} are eroarea cea mai mare ({max_error:.2f})")
            explanations.append("Acest punct 'trage' puternic gradientul Ã®n direcÈ›ia sa")
        
        # 6. PredicÈ›ie convergenÈ›Äƒ
        gradient_mag = np.sqrt(dw**2 + db**2)
        if gradient_mag < 0.01:
            explanations.append("âœ¨ CONVERGENÈšÄ‚ APROAPE! Gradientul este foarte mic")
        elif gradient_mag > 1.0:
            explanations.append("ğŸš€ ÃncÄƒ departe de optimum, gradient mare")
        
        return explanations
    
    @staticmethod
    def analyze_learning_rate(lr: float) -> List[str]:
        """AnalizeazÄƒ dacÄƒ learning rate-ul este adecvat."""
        warnings = []
        
        if lr > 0.1:
            warnings.append("âš ï¸ Learning rate FOARTE MARE! Risc de oscilaÈ›ie sau divergenÈ›Äƒ.")
        elif lr > 0.05:
            warnings.append("âš ï¸ Learning rate mare. Modelul poate oscila.")
        elif lr < 0.001:
            warnings.append("ğŸŒ Learning rate foarte mic. ConvergenÈ›a va fi lentÄƒ.")
        
        return warnings
